{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 3s 10ms/step - loss: 401.5499 - mae: 16.0647 - mse: 401.5499 - val_loss: 265.4935 - val_mae: 12.5411 - val_mse: 265.4935\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 187.6877 - mae: 10.8257 - mse: 187.6877 - val_loss: 139.9419 - val_mae: 9.7934 - val_mse: 139.9419\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 155.0835 - mae: 10.3942 - mse: 155.0835 - val_loss: 144.1886 - val_mae: 9.9833 - val_mse: 144.1886\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 155.8002 - mae: 10.4582 - mse: 155.8002 - val_loss: 142.6620 - val_mae: 10.0184 - val_mse: 142.6620\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 153.9186 - mae: 10.4332 - mse: 153.9186 - val_loss: 141.5253 - val_mae: 9.9382 - val_mse: 141.5253\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 153.8492 - mae: 10.4088 - mse: 153.8492 - val_loss: 140.3716 - val_mae: 9.9108 - val_mse: 140.3716\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 153.1149 - mae: 10.3682 - mse: 153.1149 - val_loss: 143.9983 - val_mae: 10.0648 - val_mse: 143.9983\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 153.7806 - mae: 10.3974 - mse: 153.7806 - val_loss: 140.3530 - val_mae: 9.9450 - val_mse: 140.3530\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 153.4060 - mae: 10.3979 - mse: 153.4060 - val_loss: 139.8082 - val_mae: 9.8686 - val_mse: 139.8082\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.9350 - mae: 10.3441 - mse: 151.9350 - val_loss: 140.8965 - val_mae: 9.9549 - val_mse: 140.8965\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 152.3298 - mae: 10.3621 - mse: 152.3298 - val_loss: 139.4715 - val_mae: 9.8022 - val_mse: 139.4715\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.6270 - mae: 10.3318 - mse: 151.6270 - val_loss: 140.1633 - val_mae: 9.7642 - val_mse: 140.1633\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 152.0561 - mae: 10.3415 - mse: 152.0561 - val_loss: 141.0667 - val_mae: 10.0031 - val_mse: 141.0667\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.6178 - mae: 10.3189 - mse: 151.6178 - val_loss: 143.4931 - val_mae: 10.1506 - val_mse: 143.4931\n",
      "Epoch 15/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 152.3752 - mae: 10.3749 - mse: 152.3752 - val_loss: 141.1540 - val_mae: 9.9858 - val_mse: 141.1540\n",
      "Epoch 16/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 152.0819 - mae: 10.3677 - mse: 152.0819 - val_loss: 143.4971 - val_mae: 10.1352 - val_mse: 143.4971\n",
      "Epoch 17/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.7478 - mae: 10.3423 - mse: 151.7478 - val_loss: 142.2583 - val_mae: 10.0533 - val_mse: 142.2583\n",
      "Epoch 18/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 151.7212 - mae: 10.3570 - mse: 151.7212 - val_loss: 140.1695 - val_mae: 9.8924 - val_mse: 140.1695\n",
      "Epoch 19/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.1331 - mae: 10.3086 - mse: 151.1331 - val_loss: 141.2811 - val_mae: 9.9898 - val_mse: 141.2811\n",
      "Epoch 20/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.6418 - mae: 10.3428 - mse: 151.6418 - val_loss: 139.9532 - val_mae: 9.8577 - val_mse: 139.9532\n",
      "Epoch 21/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.3499 - mae: 10.3469 - mse: 151.3499 - val_loss: 140.2755 - val_mae: 9.8990 - val_mse: 140.2755\n",
      "Epoch 22/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.5052 - mae: 10.3274 - mse: 151.5052 - val_loss: 143.4664 - val_mae: 10.1030 - val_mse: 143.4664\n",
      "Epoch 23/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.8628 - mae: 10.3247 - mse: 150.8628 - val_loss: 140.2634 - val_mae: 9.9353 - val_mse: 140.2634\n",
      "Epoch 24/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.8059 - mae: 10.3150 - mse: 150.8059 - val_loss: 140.4694 - val_mae: 9.9359 - val_mse: 140.4694\n",
      "Epoch 25/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 152.2760 - mae: 10.3601 - mse: 152.2760 - val_loss: 140.0435 - val_mae: 9.9234 - val_mse: 140.0435\n",
      "Epoch 26/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.8276 - mae: 10.3157 - mse: 150.8276 - val_loss: 146.1606 - val_mae: 10.2457 - val_mse: 146.1606\n",
      "Epoch 27/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.2787 - mae: 10.3286 - mse: 151.2787 - val_loss: 140.5479 - val_mae: 9.9786 - val_mse: 140.5479\n",
      "Epoch 28/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.5984 - mae: 10.3253 - mse: 150.5984 - val_loss: 141.2714 - val_mae: 10.0291 - val_mse: 141.2714\n",
      "Epoch 29/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.6030 - mae: 10.3557 - mse: 151.6030 - val_loss: 140.9800 - val_mae: 9.9856 - val_mse: 140.9800\n",
      "Epoch 30/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.7334 - mae: 10.3466 - mse: 151.7334 - val_loss: 142.7040 - val_mae: 10.0597 - val_mse: 142.7040\n",
      "Epoch 31/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.9227 - mae: 10.2992 - mse: 150.9227 - val_loss: 140.7034 - val_mae: 9.9522 - val_mse: 140.7034\n",
      "Epoch 32/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.6148 - mae: 10.3407 - mse: 151.6148 - val_loss: 140.5248 - val_mae: 9.9702 - val_mse: 140.5248\n",
      "Epoch 33/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.6299 - mae: 10.3117 - mse: 150.6299 - val_loss: 140.3042 - val_mae: 9.9345 - val_mse: 140.3042\n",
      "Epoch 34/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.7205 - mae: 10.3194 - mse: 150.7205 - val_loss: 140.7347 - val_mae: 9.9684 - val_mse: 140.7347\n",
      "Epoch 35/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.0281 - mae: 10.3031 - mse: 151.0281 - val_loss: 140.7332 - val_mae: 9.9663 - val_mse: 140.7332\n",
      "Epoch 36/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.4245 - mae: 10.3102 - mse: 150.4245 - val_loss: 140.1026 - val_mae: 9.8492 - val_mse: 140.1026\n",
      "Epoch 37/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.8369 - mae: 10.3321 - mse: 150.8369 - val_loss: 140.3551 - val_mae: 9.9384 - val_mse: 140.3551\n",
      "Epoch 38/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.1792 - mae: 10.3345 - mse: 151.1792 - val_loss: 139.9754 - val_mae: 9.9092 - val_mse: 139.9754\n",
      "Epoch 39/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.9170 - mae: 10.3431 - mse: 150.9170 - val_loss: 139.9323 - val_mae: 9.8741 - val_mse: 139.9323\n",
      "Epoch 40/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5365 - mae: 10.2892 - mse: 150.5365 - val_loss: 142.7911 - val_mae: 10.1036 - val_mse: 142.7911\n",
      "Epoch 41/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.4823 - mae: 10.3639 - mse: 151.4823 - val_loss: 139.9843 - val_mae: 9.9210 - val_mse: 139.9843\n",
      "Epoch 42/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.1915 - mae: 10.2713 - mse: 150.1915 - val_loss: 140.2554 - val_mae: 9.9517 - val_mse: 140.2554\n",
      "Epoch 43/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.4242 - mae: 10.3630 - mse: 151.4242 - val_loss: 141.7196 - val_mae: 10.0322 - val_mse: 141.7196\n",
      "Epoch 44/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.6066 - mae: 10.3382 - mse: 150.6066 - val_loss: 140.3331 - val_mae: 9.9293 - val_mse: 140.3331\n",
      "Epoch 45/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.9940 - mae: 10.2995 - mse: 150.9940 - val_loss: 140.5147 - val_mae: 9.9528 - val_mse: 140.5147\n",
      "Epoch 46/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.1223 - mae: 10.3334 - mse: 151.1223 - val_loss: 139.3171 - val_mae: 9.8049 - val_mse: 139.3171\n",
      "Epoch 47/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.9898 - mae: 10.3306 - mse: 150.9898 - val_loss: 139.9812 - val_mae: 9.8976 - val_mse: 139.9812\n",
      "Epoch 48/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.5555 - mae: 10.3100 - mse: 150.5555 - val_loss: 140.8011 - val_mae: 9.9914 - val_mse: 140.8011\n",
      "Epoch 49/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.0793 - mae: 10.3103 - mse: 150.0793 - val_loss: 140.6208 - val_mae: 9.9765 - val_mse: 140.6208\n",
      "Epoch 50/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3489 - mae: 10.2981 - mse: 150.3489 - val_loss: 143.4743 - val_mae: 10.1180 - val_mse: 143.4743\n",
      "Epoch 51/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5153 - mae: 10.3243 - mse: 150.5153 - val_loss: 140.1294 - val_mae: 9.9198 - val_mse: 140.1294\n",
      "Epoch 52/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5897 - mae: 10.3372 - mse: 150.5897 - val_loss: 139.3277 - val_mae: 9.8044 - val_mse: 139.3277\n",
      "Epoch 53/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5091 - mae: 10.3147 - mse: 150.5091 - val_loss: 140.9880 - val_mae: 10.0032 - val_mse: 140.9880\n",
      "Epoch 54/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5077 - mae: 10.3178 - mse: 150.5077 - val_loss: 140.6006 - val_mae: 9.9501 - val_mse: 140.6006\n",
      "Epoch 55/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3981 - mae: 10.3058 - mse: 150.3981 - val_loss: 140.4481 - val_mae: 9.9010 - val_mse: 140.4481\n",
      "Epoch 56/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.9303 - mae: 10.3157 - mse: 150.9303 - val_loss: 140.8878 - val_mae: 9.9955 - val_mse: 140.8878\n",
      "Epoch 57/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.1960 - mae: 10.3024 - mse: 150.1960 - val_loss: 139.5934 - val_mae: 9.8551 - val_mse: 139.5934\n",
      "Epoch 58/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.7946 - mae: 10.3201 - mse: 150.7946 - val_loss: 139.5367 - val_mae: 9.8406 - val_mse: 139.5367\n",
      "Epoch 59/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.3641 - mae: 10.3080 - mse: 150.3641 - val_loss: 139.4184 - val_mae: 9.7750 - val_mse: 139.4184\n",
      "Epoch 60/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.6415 - mae: 10.3311 - mse: 150.6415 - val_loss: 140.3483 - val_mae: 9.8890 - val_mse: 140.3483\n",
      "Epoch 61/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3243 - mae: 10.2983 - mse: 150.3243 - val_loss: 141.6136 - val_mae: 10.0415 - val_mse: 141.6136\n",
      "Epoch 62/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8970 - mae: 10.2940 - mse: 149.8970 - val_loss: 139.3593 - val_mae: 9.8311 - val_mse: 139.3593\n",
      "Epoch 63/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.6749 - mae: 10.3343 - mse: 150.6749 - val_loss: 139.7733 - val_mae: 9.8811 - val_mse: 139.7733\n",
      "Epoch 64/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.4328 - mae: 10.3163 - mse: 150.4328 - val_loss: 139.3053 - val_mae: 9.7689 - val_mse: 139.3053\n",
      "Epoch 65/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 151.1549 - mae: 10.3193 - mse: 151.1549 - val_loss: 142.0597 - val_mae: 10.0797 - val_mse: 142.0597\n",
      "Epoch 66/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.9205 - mae: 10.3388 - mse: 150.9205 - val_loss: 140.2828 - val_mae: 9.9389 - val_mse: 140.2828\n",
      "Epoch 67/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3994 - mae: 10.3185 - mse: 150.3994 - val_loss: 139.7431 - val_mae: 9.8822 - val_mse: 139.7431\n",
      "Epoch 68/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.4320 - mae: 10.3147 - mse: 150.4320 - val_loss: 139.3637 - val_mae: 9.7944 - val_mse: 139.3637\n",
      "Epoch 69/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5864 - mae: 10.3201 - mse: 150.5864 - val_loss: 139.7852 - val_mae: 9.7989 - val_mse: 139.7852\n",
      "Epoch 70/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.7465 - mae: 10.2807 - mse: 149.7465 - val_loss: 142.4018 - val_mae: 10.1088 - val_mse: 142.4018\n",
      "Epoch 71/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3638 - mae: 10.3263 - mse: 150.3638 - val_loss: 140.4090 - val_mae: 9.9497 - val_mse: 140.4090\n",
      "Epoch 72/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5262 - mae: 10.3079 - mse: 150.5262 - val_loss: 140.1622 - val_mae: 9.9144 - val_mse: 140.1622\n",
      "Epoch 73/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.4183 - mae: 10.3180 - mse: 150.4183 - val_loss: 141.4895 - val_mae: 10.0136 - val_mse: 141.4895\n",
      "Epoch 74/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.1723 - mae: 10.2905 - mse: 150.1723 - val_loss: 141.5724 - val_mae: 10.0536 - val_mse: 141.5724\n",
      "Epoch 75/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.9386 - mae: 10.2883 - mse: 149.9386 - val_loss: 140.4891 - val_mae: 9.9521 - val_mse: 140.4891\n",
      "Epoch 76/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5823 - mae: 10.3240 - mse: 150.5823 - val_loss: 140.4465 - val_mae: 9.9259 - val_mse: 140.4465\n",
      "Epoch 77/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2815 - mae: 10.3010 - mse: 150.2815 - val_loss: 140.6656 - val_mae: 9.9763 - val_mse: 140.6656\n",
      "Epoch 78/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2069 - mae: 10.3288 - mse: 150.2069 - val_loss: 140.0953 - val_mae: 9.9134 - val_mse: 140.0953\n",
      "Epoch 79/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.1043 - mae: 10.3044 - mse: 150.1043 - val_loss: 139.8712 - val_mae: 9.8798 - val_mse: 139.8712\n",
      "Epoch 80/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.1986 - mae: 10.3045 - mse: 150.1986 - val_loss: 139.9319 - val_mae: 9.8865 - val_mse: 139.9319\n",
      "Epoch 81/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.9265 - mae: 10.2969 - mse: 149.9265 - val_loss: 140.6039 - val_mae: 9.9376 - val_mse: 140.6039\n",
      "Epoch 82/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2328 - mae: 10.3086 - mse: 150.2328 - val_loss: 140.8383 - val_mae: 9.9392 - val_mse: 140.8383\n",
      "Epoch 83/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.2792 - mae: 10.3142 - mse: 150.2792 - val_loss: 141.2719 - val_mae: 10.0176 - val_mse: 141.2719\n",
      "Epoch 84/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2954 - mae: 10.3232 - mse: 150.2954 - val_loss: 141.2307 - val_mae: 9.9345 - val_mse: 141.2307\n",
      "Epoch 85/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.4978 - mae: 10.2891 - mse: 150.4978 - val_loss: 141.1796 - val_mae: 10.0191 - val_mse: 141.1796\n",
      "Epoch 86/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2843 - mae: 10.3132 - mse: 150.2843 - val_loss: 140.9115 - val_mae: 9.9879 - val_mse: 140.9115\n",
      "Epoch 87/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3393 - mae: 10.3094 - mse: 150.3393 - val_loss: 143.2487 - val_mae: 10.1364 - val_mse: 143.2487\n",
      "Epoch 88/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3389 - mae: 10.3164 - mse: 150.3389 - val_loss: 140.1020 - val_mae: 9.8650 - val_mse: 140.1020\n",
      "Epoch 89/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.6278 - mae: 10.3217 - mse: 150.6278 - val_loss: 140.0348 - val_mae: 9.8951 - val_mse: 140.0348\n",
      "Epoch 90/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.6046 - mae: 10.3087 - mse: 150.6046 - val_loss: 140.1814 - val_mae: 9.9169 - val_mse: 140.1814\n",
      "Epoch 91/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2579 - mae: 10.3142 - mse: 150.2579 - val_loss: 139.6410 - val_mae: 9.7933 - val_mse: 139.6410\n",
      "Epoch 92/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2368 - mae: 10.2966 - mse: 150.2368 - val_loss: 140.4378 - val_mae: 9.9598 - val_mse: 140.4378\n",
      "Epoch 93/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3457 - mae: 10.3130 - mse: 150.3457 - val_loss: 139.8631 - val_mae: 9.8821 - val_mse: 139.8631\n",
      "Epoch 94/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8903 - mae: 10.3077 - mse: 149.8903 - val_loss: 142.2440 - val_mae: 10.0832 - val_mse: 142.2440\n",
      "Epoch 95/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2817 - mae: 10.3113 - mse: 150.2817 - val_loss: 140.5178 - val_mae: 9.8612 - val_mse: 140.5178\n",
      "Epoch 96/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.0489 - mae: 10.2931 - mse: 150.0489 - val_loss: 144.4375 - val_mae: 10.2200 - val_mse: 144.4375\n",
      "Epoch 97/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8847 - mae: 10.2943 - mse: 149.8847 - val_loss: 140.4516 - val_mae: 9.9558 - val_mse: 140.4516\n",
      "Epoch 98/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.7262 - mae: 10.2890 - mse: 149.7262 - val_loss: 142.1229 - val_mae: 10.0773 - val_mse: 142.1229\n",
      "Epoch 99/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.5638 - mae: 10.2890 - mse: 149.5638 - val_loss: 139.9472 - val_mae: 9.8842 - val_mse: 139.9472\n",
      "Epoch 100/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.1857 - mae: 10.2971 - mse: 150.1857 - val_loss: 140.5954 - val_mae: 9.9622 - val_mse: 140.5954\n",
      "Epoch 101/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.0623 - mae: 10.3015 - mse: 150.0623 - val_loss: 140.4183 - val_mae: 9.9591 - val_mse: 140.4183\n",
      "Epoch 102/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.6155 - mae: 10.2951 - mse: 149.6155 - val_loss: 141.7505 - val_mae: 10.0466 - val_mse: 141.7505\n",
      "Epoch 103/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.8252 - mae: 10.3297 - mse: 150.8252 - val_loss: 140.7766 - val_mae: 9.9905 - val_mse: 140.7766\n",
      "Epoch 104/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8427 - mae: 10.3147 - mse: 149.8427 - val_loss: 140.5075 - val_mae: 9.9605 - val_mse: 140.5075\n",
      "Epoch 105/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.2706 - mae: 10.2777 - mse: 149.2706 - val_loss: 140.0506 - val_mae: 9.9256 - val_mse: 140.0506\n",
      "Epoch 106/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8663 - mae: 10.2840 - mse: 149.8663 - val_loss: 140.9618 - val_mae: 10.0139 - val_mse: 140.9618\n",
      "Epoch 107/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2658 - mae: 10.3361 - mse: 150.2658 - val_loss: 143.4517 - val_mae: 10.1585 - val_mse: 143.4517\n",
      "Epoch 108/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5207 - mae: 10.3172 - mse: 150.5207 - val_loss: 140.0807 - val_mae: 9.8965 - val_mse: 140.0807\n",
      "Epoch 109/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.8657 - mae: 10.2981 - mse: 149.8657 - val_loss: 140.9201 - val_mae: 9.9855 - val_mse: 140.9201\n",
      "Epoch 110/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2520 - mae: 10.3077 - mse: 150.2520 - val_loss: 140.5683 - val_mae: 9.9775 - val_mse: 140.5683\n",
      "Epoch 111/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.9834 - mae: 10.3049 - mse: 149.9834 - val_loss: 141.5173 - val_mae: 9.9986 - val_mse: 141.5173\n",
      "Epoch 112/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.5807 - mae: 10.2888 - mse: 149.5807 - val_loss: 139.8065 - val_mae: 9.8065 - val_mse: 139.8065\n",
      "Epoch 113/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.0316 - mae: 10.2940 - mse: 150.0316 - val_loss: 141.3494 - val_mae: 9.9673 - val_mse: 141.3494\n",
      "Epoch 114/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.6062 - mae: 10.3284 - mse: 150.6062 - val_loss: 140.7960 - val_mae: 9.9858 - val_mse: 140.7960\n",
      "Epoch 115/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.9197 - mae: 10.3171 - mse: 149.9197 - val_loss: 140.5974 - val_mae: 9.9612 - val_mse: 140.5974\n",
      "Epoch 116/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.5971 - mae: 10.2982 - mse: 149.5971 - val_loss: 139.6230 - val_mae: 9.8320 - val_mse: 139.6230\n",
      "Epoch 117/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2333 - mae: 10.2981 - mse: 150.2333 - val_loss: 141.1296 - val_mae: 10.0145 - val_mse: 141.1296\n",
      "Epoch 118/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3087 - mae: 10.3203 - mse: 150.3087 - val_loss: 140.8863 - val_mae: 9.9976 - val_mse: 140.8863\n",
      "Epoch 119/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.6588 - mae: 10.3305 - mse: 150.6588 - val_loss: 143.1389 - val_mae: 10.1380 - val_mse: 143.1389\n",
      "Epoch 120/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.7210 - mae: 10.2825 - mse: 149.7210 - val_loss: 142.1215 - val_mae: 10.0903 - val_mse: 142.1215\n",
      "Epoch 121/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.3723 - mae: 10.3379 - mse: 150.3723 - val_loss: 139.8671 - val_mae: 9.8865 - val_mse: 139.8671\n",
      "Epoch 122/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.2000 - mae: 10.2701 - mse: 149.2000 - val_loss: 142.4465 - val_mae: 10.1061 - val_mse: 142.4465\n",
      "Epoch 123/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.1894 - mae: 10.3282 - mse: 150.1894 - val_loss: 141.4270 - val_mae: 10.0436 - val_mse: 141.4270\n",
      "Epoch 124/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.7683 - mae: 10.2944 - mse: 149.7683 - val_loss: 140.1565 - val_mae: 9.9251 - val_mse: 140.1565\n",
      "Epoch 125/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.7842 - mae: 10.2878 - mse: 149.7842 - val_loss: 141.3036 - val_mae: 10.0278 - val_mse: 141.3036\n",
      "Epoch 126/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.0359 - mae: 10.3269 - mse: 150.0359 - val_loss: 140.3936 - val_mae: 9.9320 - val_mse: 140.3936\n",
      "Epoch 127/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.8607 - mae: 10.2928 - mse: 149.8607 - val_loss: 140.7479 - val_mae: 9.9778 - val_mse: 140.7479\n",
      "Epoch 128/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8534 - mae: 10.3030 - mse: 149.8534 - val_loss: 141.1028 - val_mae: 9.9991 - val_mse: 141.1028\n",
      "Epoch 129/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.6509 - mae: 10.3153 - mse: 149.6509 - val_loss: 140.0761 - val_mae: 9.8995 - val_mse: 140.0761\n",
      "Epoch 130/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.9687 - mae: 10.3026 - mse: 149.9687 - val_loss: 140.2047 - val_mae: 9.9058 - val_mse: 140.2047\n",
      "Epoch 131/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8833 - mae: 10.3085 - mse: 149.8833 - val_loss: 141.3543 - val_mae: 9.9889 - val_mse: 141.3543\n",
      "Epoch 132/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8407 - mae: 10.2907 - mse: 149.8407 - val_loss: 139.8710 - val_mae: 9.8818 - val_mse: 139.8710\n",
      "Epoch 133/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.0077 - mae: 10.2884 - mse: 150.0077 - val_loss: 140.4325 - val_mae: 9.9300 - val_mse: 140.4325\n",
      "Epoch 134/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.3112 - mae: 10.2764 - mse: 149.3112 - val_loss: 140.7510 - val_mae: 9.9921 - val_mse: 140.7510\n",
      "Epoch 135/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.7045 - mae: 10.2968 - mse: 149.7045 - val_loss: 140.6418 - val_mae: 9.9657 - val_mse: 140.6418\n",
      "Epoch 136/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.4447 - mae: 10.2989 - mse: 149.4447 - val_loss: 139.3924 - val_mae: 9.8294 - val_mse: 139.3924\n",
      "Epoch 137/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8989 - mae: 10.3126 - mse: 149.8989 - val_loss: 141.8534 - val_mae: 10.0648 - val_mse: 141.8534\n",
      "Epoch 138/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2214 - mae: 10.2951 - mse: 150.2214 - val_loss: 141.5320 - val_mae: 10.0319 - val_mse: 141.5320\n",
      "Epoch 139/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5400 - mae: 10.3173 - mse: 150.5400 - val_loss: 139.5635 - val_mae: 9.8215 - val_mse: 139.5635\n",
      "Epoch 140/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.0553 - mae: 10.2973 - mse: 150.0553 - val_loss: 140.4234 - val_mae: 9.9558 - val_mse: 140.4234\n",
      "Epoch 141/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.5076 - mae: 10.3399 - mse: 150.5076 - val_loss: 140.1914 - val_mae: 9.9245 - val_mse: 140.1914\n",
      "Epoch 142/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.9214 - mae: 10.2958 - mse: 149.9214 - val_loss: 141.1078 - val_mae: 9.9892 - val_mse: 141.1078\n",
      "Epoch 143/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.9259 - mae: 10.2998 - mse: 149.9259 - val_loss: 140.0222 - val_mae: 9.9271 - val_mse: 140.0222\n",
      "Epoch 144/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.2190 - mae: 10.3143 - mse: 150.2190 - val_loss: 140.4173 - val_mae: 9.9594 - val_mse: 140.4173\n",
      "Epoch 145/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 149.8656 - mae: 10.2870 - mse: 149.8656 - val_loss: 141.4757 - val_mae: 10.0264 - val_mse: 141.4757\n",
      "Epoch 146/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.0826 - mae: 10.2954 - mse: 150.0826 - val_loss: 140.4372 - val_mae: 9.9520 - val_mse: 140.4372\n",
      "Epoch 147/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.2009 - mae: 10.3040 - mse: 150.2009 - val_loss: 140.1863 - val_mae: 9.9341 - val_mse: 140.1863\n",
      "Epoch 148/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.0885 - mae: 10.3163 - mse: 150.0885 - val_loss: 139.5112 - val_mae: 9.8371 - val_mse: 139.5112\n",
      "Epoch 149/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.8428 - mae: 10.3126 - mse: 149.8428 - val_loss: 139.7015 - val_mae: 9.8806 - val_mse: 139.7015\n",
      "Epoch 150/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.1221 - mae: 10.3089 - mse: 150.1221 - val_loss: 141.5117 - val_mae: 10.0027 - val_mse: 141.5117\n",
      "Epoch 151/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.1107 - mae: 10.3023 - mse: 150.1107 - val_loss: 140.9347 - val_mae: 10.0036 - val_mse: 140.9347\n",
      "Epoch 152/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.0312 - mae: 10.3066 - mse: 150.0312 - val_loss: 140.1875 - val_mae: 9.8342 - val_mse: 140.1875\n",
      "Epoch 153/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 150.1169 - mae: 10.2880 - mse: 150.1169 - val_loss: 142.0816 - val_mae: 10.0794 - val_mse: 142.0816\n",
      "Epoch 154/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.9432 - mae: 10.3235 - mse: 149.9432 - val_loss: 139.9236 - val_mae: 9.8545 - val_mse: 139.9236\n",
      "Epoch 155/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 150.1035 - mae: 10.2918 - mse: 150.1035 - val_loss: 141.6325 - val_mae: 10.0637 - val_mse: 141.6325\n",
      "Epoch 156/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.2908 - mae: 10.3245 - mse: 150.2908 - val_loss: 140.6540 - val_mae: 9.9824 - val_mse: 140.6540\n",
      "Epoch 157/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 150.0570 - mae: 10.3191 - mse: 150.0570 - val_loss: 140.6931 - val_mae: 9.9819 - val_mse: 140.6931\n",
      "Epoch 158/200\n",
      "188/188 [==============================] - 3s 14ms/step - loss: 149.8345 - mae: 10.2879 - mse: 149.8345 - val_loss: 140.1670 - val_mae: 9.9293 - val_mse: 140.1670\n",
      "Epoch 159/200\n",
      "188/188 [==============================] - 3s 13ms/step - loss: 149.7748 - mae: 10.2968 - mse: 149.7748 - val_loss: 140.4073 - val_mae: 9.9720 - val_mse: 140.4073\n",
      "Epoch 160/200\n",
      "188/188 [==============================] - 3s 13ms/step - loss: 150.1624 - mae: 10.3122 - mse: 150.1624 - val_loss: 141.3709 - val_mae: 10.0352 - val_mse: 141.3709\n",
      "Epoch 161/200\n",
      "188/188 [==============================] - 3s 13ms/step - loss: 149.8824 - mae: 10.3048 - mse: 149.8824 - val_loss: 140.2063 - val_mae: 9.9461 - val_mse: 140.2063\n",
      "Epoch 162/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.8699 - mae: 10.2948 - mse: 149.8699 - val_loss: 141.3334 - val_mae: 10.0078 - val_mse: 141.3334\n",
      "Epoch 163/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.5034 - mae: 10.2930 - mse: 149.5034 - val_loss: 140.8549 - val_mae: 9.9848 - val_mse: 140.8549\n",
      "Epoch 164/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 150.1905 - mae: 10.3153 - mse: 150.1905 - val_loss: 140.6058 - val_mae: 9.9412 - val_mse: 140.6058\n",
      "Epoch 165/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 149.3384 - mae: 10.2995 - mse: 149.3384 - val_loss: 140.6532 - val_mae: 9.9608 - val_mse: 140.6532\n",
      "Epoch 166/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.3988 - mae: 10.3000 - mse: 149.3988 - val_loss: 140.1812 - val_mae: 9.8967 - val_mse: 140.1812\n",
      "Epoch 167/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.7139 - mae: 10.3047 - mse: 149.7139 - val_loss: 141.3897 - val_mae: 10.0226 - val_mse: 141.3897\n",
      "Epoch 168/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 149.2795 - mae: 10.2708 - mse: 149.2795 - val_loss: 140.4233 - val_mae: 9.9410 - val_mse: 140.4233\n",
      "Epoch 169/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.8880 - mae: 10.2926 - mse: 149.8880 - val_loss: 139.9045 - val_mae: 9.8967 - val_mse: 139.9045\n",
      "Epoch 170/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 149.9237 - mae: 10.2974 - mse: 149.9237 - val_loss: 140.1584 - val_mae: 9.9251 - val_mse: 140.1584\n",
      "Epoch 171/200\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 150.1469 - mae: 10.3179 - mse: 150.1469 - val_loss: 140.3843 - val_mae: 9.9160 - val_mse: 140.3843\n",
      "Epoch 172/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.9722 - mae: 10.2988 - mse: 149.9722 - val_loss: 142.2728 - val_mae: 10.0876 - val_mse: 142.2728\n",
      "Epoch 173/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.7257 - mae: 10.3022 - mse: 149.7257 - val_loss: 141.2543 - val_mae: 10.0286 - val_mse: 141.2543\n",
      "Epoch 174/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.9082 - mae: 10.3098 - mse: 149.9082 - val_loss: 140.5041 - val_mae: 9.9505 - val_mse: 140.5041\n",
      "Epoch 175/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.8069 - mae: 10.3022 - mse: 149.8069 - val_loss: 139.9651 - val_mae: 9.8753 - val_mse: 139.9651\n",
      "Epoch 176/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.4836 - mae: 10.2919 - mse: 149.4836 - val_loss: 139.6188 - val_mae: 9.7951 - val_mse: 139.6188\n",
      "Epoch 177/200\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 149.8442 - mae: 10.3093 - mse: 149.8442 - val_loss: 140.0482 - val_mae: 9.8778 - val_mse: 140.0482\n",
      "Epoch 178/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 149.6537 - mae: 10.2954 - mse: 149.6537 - val_loss: 140.7949 - val_mae: 9.9851 - val_mse: 140.7949\n",
      "Epoch 179/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.7602 - mae: 10.2997 - mse: 149.7602 - val_loss: 139.9263 - val_mae: 9.8895 - val_mse: 139.9263\n",
      "Epoch 180/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.2549 - mae: 10.2887 - mse: 149.2549 - val_loss: 141.1082 - val_mae: 9.9961 - val_mse: 141.1082\n",
      "Epoch 181/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.6491 - mae: 10.2961 - mse: 149.6491 - val_loss: 140.5548 - val_mae: 9.9461 - val_mse: 140.5548\n",
      "Epoch 182/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.8728 - mae: 10.2981 - mse: 149.8728 - val_loss: 140.0810 - val_mae: 9.9100 - val_mse: 140.0810\n",
      "Epoch 183/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.6359 - mae: 10.2978 - mse: 149.6359 - val_loss: 139.9358 - val_mae: 9.8496 - val_mse: 139.9358\n",
      "Epoch 184/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.3280 - mae: 10.3078 - mse: 150.3280 - val_loss: 141.8242 - val_mae: 10.0665 - val_mse: 141.8242\n",
      "Epoch 185/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 150.0026 - mae: 10.3107 - mse: 150.0026 - val_loss: 140.5314 - val_mae: 9.9697 - val_mse: 140.5314\n",
      "Epoch 186/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.6050 - mae: 10.2902 - mse: 149.6050 - val_loss: 140.0155 - val_mae: 9.9086 - val_mse: 140.0155\n",
      "Epoch 187/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.5469 - mae: 10.2960 - mse: 149.5469 - val_loss: 139.8515 - val_mae: 9.8624 - val_mse: 139.8515\n",
      "Epoch 188/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.8991 - mae: 10.3028 - mse: 149.8991 - val_loss: 141.4562 - val_mae: 10.0405 - val_mse: 141.4562\n",
      "Epoch 189/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.0633 - mae: 10.2809 - mse: 149.0633 - val_loss: 140.2356 - val_mae: 9.9378 - val_mse: 140.2356\n",
      "Epoch 190/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.9036 - mae: 10.3138 - mse: 149.9036 - val_loss: 139.5704 - val_mae: 9.8474 - val_mse: 139.5704\n",
      "Epoch 191/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.6366 - mae: 10.2818 - mse: 149.6366 - val_loss: 140.3183 - val_mae: 9.9507 - val_mse: 140.3183\n",
      "Epoch 192/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 149.9020 - mae: 10.2966 - mse: 149.9020 - val_loss: 139.6281 - val_mae: 9.8147 - val_mse: 139.6281\n",
      "Epoch 193/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.7981 - mae: 10.2970 - mse: 149.7981 - val_loss: 140.4017 - val_mae: 9.9526 - val_mse: 140.4017\n",
      "Epoch 194/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.0134 - mae: 10.3079 - mse: 150.0134 - val_loss: 139.9106 - val_mae: 9.9073 - val_mse: 139.9106\n",
      "Epoch 195/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.9186 - mae: 10.3059 - mse: 149.9186 - val_loss: 140.1289 - val_mae: 9.9150 - val_mse: 140.1289\n",
      "Epoch 196/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.8713 - mae: 10.2966 - mse: 149.8713 - val_loss: 141.2580 - val_mae: 10.0292 - val_mse: 141.2580\n",
      "Epoch 197/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.6406 - mae: 10.3050 - mse: 149.6406 - val_loss: 141.3583 - val_mae: 10.0283 - val_mse: 141.3583\n",
      "Epoch 198/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 150.3441 - mae: 10.3207 - mse: 150.3441 - val_loss: 140.6190 - val_mae: 9.9791 - val_mse: 140.6190\n",
      "Epoch 199/200\n",
      "188/188 [==============================] - 2s 10ms/step - loss: 150.1307 - mae: 10.3059 - mse: 150.1307 - val_loss: 140.1774 - val_mae: 9.9229 - val_mse: 140.1774\n",
      "Epoch 200/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 149.8161 - mae: 10.2989 - mse: 149.8161 - val_loss: 140.6748 - val_mae: 9.9765 - val_mse: 140.6748\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e35cf6d8b0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression']]\n",
    "y = data['Depression.1']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step\n",
      "Predicted Depression: 18.25\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.50, 42, 30]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Depression: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Depression_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n",
      "Predicted Depression: 18.25\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Depression_1.h5')\n",
    "\n",
    "x_in = [[0.50, 42, 30]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Depression: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 4s 10ms/step - loss: 196.8955 - mae: 10.0918 - mse: 196.8955 - val_loss: 179.2321 - val_mae: 9.5916 - val_mse: 179.2321\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 116.6944 - mae: 8.3101 - mse: 116.6944 - val_loss: 126.4168 - val_mae: 9.1307 - val_mse: 126.4168\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 106.7311 - mae: 8.3414 - mse: 106.7311 - val_loss: 107.6379 - val_mae: 8.2485 - val_mse: 107.6379\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.7386 - mae: 8.3164 - mse: 105.7386 - val_loss: 106.4005 - val_mae: 8.2287 - val_mse: 106.4005\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 105.4477 - mae: 8.2875 - mse: 105.4477 - val_loss: 107.8110 - val_mae: 8.3014 - val_mse: 107.8110\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 104.6715 - mae: 8.2675 - mse: 104.6715 - val_loss: 107.5667 - val_mae: 8.2635 - val_mse: 107.5667\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 104.4451 - mae: 8.2786 - mse: 104.4451 - val_loss: 106.0842 - val_mae: 8.2782 - val_mse: 106.0842\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.5302 - mae: 8.2805 - mse: 104.5302 - val_loss: 106.0624 - val_mae: 8.2007 - val_mse: 106.0624\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 105.1884 - mae: 8.3296 - mse: 105.1884 - val_loss: 107.1183 - val_mae: 8.2359 - val_mse: 107.1183\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.4529 - mae: 8.2716 - mse: 104.4529 - val_loss: 105.9822 - val_mae: 8.2182 - val_mse: 105.9822\n",
      "Epoch 11/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.9997 - mae: 8.3031 - mse: 104.9997 - val_loss: 107.4758 - val_mae: 8.2499 - val_mse: 107.4758\n",
      "Epoch 12/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 104.4397 - mae: 8.2861 - mse: 104.4397 - val_loss: 106.3568 - val_mae: 8.2281 - val_mse: 106.3568\n",
      "Epoch 13/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.1528 - mae: 8.2636 - mse: 104.1528 - val_loss: 107.7974 - val_mae: 8.1808 - val_mse: 107.7974\n",
      "Epoch 14/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.8951 - mae: 8.2462 - mse: 103.8951 - val_loss: 105.9109 - val_mae: 8.2197 - val_mse: 105.9109\n",
      "Epoch 15/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.9772 - mae: 8.2622 - mse: 103.9772 - val_loss: 105.9064 - val_mae: 8.2058 - val_mse: 105.9064\n",
      "Epoch 16/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 104.0965 - mae: 8.2574 - mse: 104.0965 - val_loss: 105.7070 - val_mae: 8.1939 - val_mse: 105.7070\n",
      "Epoch 17/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 104.2468 - mae: 8.2557 - mse: 104.2468 - val_loss: 106.5055 - val_mae: 8.2694 - val_mse: 106.5055\n",
      "Epoch 18/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.6417 - mae: 8.2591 - mse: 103.6417 - val_loss: 105.9035 - val_mae: 8.3022 - val_mse: 105.9035\n",
      "Epoch 19/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.8356 - mae: 8.2711 - mse: 103.8356 - val_loss: 105.9141 - val_mae: 8.2252 - val_mse: 105.9141\n",
      "Epoch 20/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.6526 - mae: 8.2475 - mse: 103.6526 - val_loss: 106.3099 - val_mae: 8.2086 - val_mse: 106.3099\n",
      "Epoch 21/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.1053 - mae: 8.2692 - mse: 104.1053 - val_loss: 105.7188 - val_mae: 8.2324 - val_mse: 105.7188\n",
      "Epoch 22/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 104.0390 - mae: 8.2570 - mse: 104.0390 - val_loss: 108.4115 - val_mae: 8.1877 - val_mse: 108.4115\n",
      "Epoch 23/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.6953 - mae: 8.2732 - mse: 103.6953 - val_loss: 106.8719 - val_mae: 8.2136 - val_mse: 106.8719\n",
      "Epoch 24/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.2202 - mae: 8.2461 - mse: 104.2202 - val_loss: 105.1635 - val_mae: 8.2788 - val_mse: 105.1635\n",
      "Epoch 25/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.4268 - mae: 8.2198 - mse: 103.4268 - val_loss: 107.0861 - val_mae: 8.2665 - val_mse: 107.0861\n",
      "Epoch 26/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.1619 - mae: 8.2644 - mse: 104.1619 - val_loss: 106.3181 - val_mae: 8.1461 - val_mse: 106.3181\n",
      "Epoch 27/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.0707 - mae: 8.2638 - mse: 104.0707 - val_loss: 105.6734 - val_mae: 8.2026 - val_mse: 105.6734\n",
      "Epoch 28/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.7130 - mae: 8.2608 - mse: 103.7130 - val_loss: 105.6562 - val_mae: 8.1657 - val_mse: 105.6562\n",
      "Epoch 29/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.6613 - mae: 8.2385 - mse: 103.6613 - val_loss: 105.2638 - val_mae: 8.2538 - val_mse: 105.2638\n",
      "Epoch 30/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.6306 - mae: 8.2522 - mse: 103.6306 - val_loss: 106.0540 - val_mae: 8.1682 - val_mse: 106.0540\n",
      "Epoch 31/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5365 - mae: 8.2499 - mse: 103.5365 - val_loss: 105.6256 - val_mae: 8.1816 - val_mse: 105.6256\n",
      "Epoch 32/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.8819 - mae: 8.2611 - mse: 103.8819 - val_loss: 105.8493 - val_mae: 8.2574 - val_mse: 105.8493\n",
      "Epoch 33/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.4502 - mae: 8.2517 - mse: 103.4502 - val_loss: 105.7688 - val_mae: 8.1877 - val_mse: 105.7688\n",
      "Epoch 34/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.8372 - mae: 8.2446 - mse: 103.8372 - val_loss: 106.0596 - val_mae: 8.2116 - val_mse: 106.0596\n",
      "Epoch 35/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.4675 - mae: 8.2215 - mse: 103.4675 - val_loss: 105.4958 - val_mae: 8.3510 - val_mse: 105.4958\n",
      "Epoch 36/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.8659 - mae: 8.2721 - mse: 103.8659 - val_loss: 105.5948 - val_mae: 8.2126 - val_mse: 105.5948\n",
      "Epoch 37/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1989 - mae: 8.2262 - mse: 103.1989 - val_loss: 105.5603 - val_mae: 8.2703 - val_mse: 105.5603\n",
      "Epoch 38/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 104.2845 - mae: 8.2795 - mse: 104.2845 - val_loss: 105.5159 - val_mae: 8.2155 - val_mse: 105.5159\n",
      "Epoch 39/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5021 - mae: 8.2508 - mse: 103.5021 - val_loss: 106.1619 - val_mae: 8.1687 - val_mse: 106.1619\n",
      "Epoch 40/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5716 - mae: 8.2413 - mse: 103.5716 - val_loss: 106.1011 - val_mae: 8.2303 - val_mse: 106.1011\n",
      "Epoch 41/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.5053 - mae: 8.2473 - mse: 103.5053 - val_loss: 106.1097 - val_mae: 8.3342 - val_mse: 106.1097\n",
      "Epoch 42/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.7751 - mae: 8.2647 - mse: 103.7751 - val_loss: 106.5557 - val_mae: 8.1698 - val_mse: 106.5557\n",
      "Epoch 43/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3617 - mae: 8.2403 - mse: 103.3617 - val_loss: 105.6450 - val_mae: 8.2090 - val_mse: 105.6450\n",
      "Epoch 44/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.7019 - mae: 8.2497 - mse: 103.7019 - val_loss: 105.4212 - val_mae: 8.2300 - val_mse: 105.4212\n",
      "Epoch 45/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.7500 - mae: 8.2668 - mse: 103.7500 - val_loss: 105.9810 - val_mae: 8.1886 - val_mse: 105.9810\n",
      "Epoch 46/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5998 - mae: 8.2311 - mse: 103.5998 - val_loss: 106.2310 - val_mae: 8.1886 - val_mse: 106.2310\n",
      "Epoch 47/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4895 - mae: 8.2496 - mse: 103.4895 - val_loss: 105.7805 - val_mae: 8.2314 - val_mse: 105.7805\n",
      "Epoch 48/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1974 - mae: 8.2352 - mse: 103.1974 - val_loss: 106.1046 - val_mae: 8.2078 - val_mse: 106.1046\n",
      "Epoch 49/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.3484 - mae: 8.2459 - mse: 103.3484 - val_loss: 106.3643 - val_mae: 8.2219 - val_mse: 106.3643\n",
      "Epoch 50/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4126 - mae: 8.2218 - mse: 103.4126 - val_loss: 106.3400 - val_mae: 8.2221 - val_mse: 106.3400\n",
      "Epoch 51/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5939 - mae: 8.2492 - mse: 103.5939 - val_loss: 105.9423 - val_mae: 8.1837 - val_mse: 105.9423\n",
      "Epoch 52/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3966 - mae: 8.2492 - mse: 103.3966 - val_loss: 105.8047 - val_mae: 8.2214 - val_mse: 105.8047\n",
      "Epoch 53/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5813 - mae: 8.2448 - mse: 103.5813 - val_loss: 105.6137 - val_mae: 8.2236 - val_mse: 105.6137\n",
      "Epoch 54/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4544 - mae: 8.2495 - mse: 103.4544 - val_loss: 106.1169 - val_mae: 8.1740 - val_mse: 106.1169\n",
      "Epoch 55/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4474 - mae: 8.2542 - mse: 103.4474 - val_loss: 106.3288 - val_mae: 8.1735 - val_mse: 106.3288\n",
      "Epoch 56/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4022 - mae: 8.2288 - mse: 103.4022 - val_loss: 106.6236 - val_mae: 8.1401 - val_mse: 106.6236\n",
      "Epoch 57/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3832 - mae: 8.2447 - mse: 103.3832 - val_loss: 105.3480 - val_mae: 8.2620 - val_mse: 105.3480\n",
      "Epoch 58/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.7762 - mae: 8.2441 - mse: 103.7762 - val_loss: 106.0530 - val_mae: 8.1717 - val_mse: 106.0530\n",
      "Epoch 59/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2944 - mae: 8.2384 - mse: 103.2944 - val_loss: 105.7539 - val_mae: 8.1695 - val_mse: 105.7539\n",
      "Epoch 60/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5686 - mae: 8.2313 - mse: 103.5686 - val_loss: 105.4317 - val_mae: 8.2711 - val_mse: 105.4317\n",
      "Epoch 61/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.7073 - mae: 8.2619 - mse: 103.7073 - val_loss: 106.0739 - val_mae: 8.1701 - val_mse: 106.0739\n",
      "Epoch 62/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.7673 - mae: 8.2515 - mse: 103.7673 - val_loss: 105.4096 - val_mae: 8.2498 - val_mse: 105.4096\n",
      "Epoch 63/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3736 - mae: 8.2415 - mse: 103.3736 - val_loss: 105.6906 - val_mae: 8.1907 - val_mse: 105.6906\n",
      "Epoch 64/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2727 - mae: 8.2493 - mse: 103.2727 - val_loss: 106.3178 - val_mae: 8.2193 - val_mse: 106.3178\n",
      "Epoch 65/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5684 - mae: 8.2536 - mse: 103.5684 - val_loss: 105.6009 - val_mae: 8.1999 - val_mse: 105.6009\n",
      "Epoch 66/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0691 - mae: 8.2460 - mse: 103.0691 - val_loss: 106.1004 - val_mae: 8.1703 - val_mse: 106.1004\n",
      "Epoch 67/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3761 - mae: 8.2257 - mse: 103.3761 - val_loss: 105.4582 - val_mae: 8.2834 - val_mse: 105.4582\n",
      "Epoch 68/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1368 - mae: 8.2414 - mse: 103.1368 - val_loss: 105.5693 - val_mae: 8.2390 - val_mse: 105.5693\n",
      "Epoch 69/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1861 - mae: 8.2236 - mse: 103.1861 - val_loss: 105.5230 - val_mae: 8.2154 - val_mse: 105.5230\n",
      "Epoch 70/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1369 - mae: 8.2341 - mse: 103.1369 - val_loss: 106.5719 - val_mae: 8.1474 - val_mse: 106.5719\n",
      "Epoch 71/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.3246 - mae: 8.2431 - mse: 103.3246 - val_loss: 105.5117 - val_mae: 8.2037 - val_mse: 105.5117\n",
      "Epoch 72/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3155 - mae: 8.2233 - mse: 103.3155 - val_loss: 105.5135 - val_mae: 8.2405 - val_mse: 105.5135\n",
      "Epoch 73/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3856 - mae: 8.2474 - mse: 103.3856 - val_loss: 106.3768 - val_mae: 8.2362 - val_mse: 106.3768\n",
      "Epoch 74/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5540 - mae: 8.2445 - mse: 103.5540 - val_loss: 105.7136 - val_mae: 8.2024 - val_mse: 105.7136\n",
      "Epoch 75/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.6031 - mae: 8.2602 - mse: 103.6031 - val_loss: 106.5667 - val_mae: 8.1821 - val_mse: 106.5667\n",
      "Epoch 76/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4654 - mae: 8.2400 - mse: 103.4654 - val_loss: 106.0414 - val_mae: 8.2281 - val_mse: 106.0414\n",
      "Epoch 77/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4475 - mae: 8.2423 - mse: 103.4475 - val_loss: 105.7732 - val_mae: 8.2068 - val_mse: 105.7732\n",
      "Epoch 78/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1831 - mae: 8.2360 - mse: 103.1831 - val_loss: 105.7078 - val_mae: 8.1950 - val_mse: 105.7078\n",
      "Epoch 79/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5543 - mae: 8.2370 - mse: 103.5543 - val_loss: 105.4438 - val_mae: 8.2648 - val_mse: 105.4438\n",
      "Epoch 80/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.4511 - mae: 8.2471 - mse: 103.4511 - val_loss: 105.5769 - val_mae: 8.2170 - val_mse: 105.5769\n",
      "Epoch 81/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5012 - mae: 8.2430 - mse: 103.5012 - val_loss: 105.4486 - val_mae: 8.2451 - val_mse: 105.4486\n",
      "Epoch 82/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4244 - mae: 8.2333 - mse: 103.4244 - val_loss: 105.5931 - val_mae: 8.3169 - val_mse: 105.5931\n",
      "Epoch 83/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1856 - mae: 8.2408 - mse: 103.1856 - val_loss: 105.8554 - val_mae: 8.2540 - val_mse: 105.8554\n",
      "Epoch 84/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4015 - mae: 8.2538 - mse: 103.4015 - val_loss: 105.5744 - val_mae: 8.2436 - val_mse: 105.5744\n",
      "Epoch 85/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.6845 - mae: 8.2431 - mse: 103.6845 - val_loss: 105.4171 - val_mae: 8.2513 - val_mse: 105.4171\n",
      "Epoch 86/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2877 - mae: 8.2408 - mse: 103.2877 - val_loss: 107.0429 - val_mae: 8.1421 - val_mse: 107.0429\n",
      "Epoch 87/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9463 - mae: 8.2184 - mse: 102.9463 - val_loss: 105.4429 - val_mae: 8.2639 - val_mse: 105.4429\n",
      "Epoch 88/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2352 - mae: 8.2402 - mse: 103.2352 - val_loss: 105.8624 - val_mae: 8.1874 - val_mse: 105.8624\n",
      "Epoch 89/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2087 - mae: 8.2474 - mse: 103.2087 - val_loss: 105.5911 - val_mae: 8.1936 - val_mse: 105.5911\n",
      "Epoch 90/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2800 - mae: 8.2406 - mse: 103.2800 - val_loss: 105.5995 - val_mae: 8.2199 - val_mse: 105.5995\n",
      "Epoch 91/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0130 - mae: 8.2014 - mse: 103.0130 - val_loss: 106.1080 - val_mae: 8.3649 - val_mse: 106.1080\n",
      "Epoch 92/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.5544 - mae: 8.2642 - mse: 103.5544 - val_loss: 105.2448 - val_mae: 8.2738 - val_mse: 105.2448\n",
      "Epoch 93/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1944 - mae: 8.2387 - mse: 103.1944 - val_loss: 105.4267 - val_mae: 8.2254 - val_mse: 105.4267\n",
      "Epoch 94/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3394 - mae: 8.2304 - mse: 103.3394 - val_loss: 105.2525 - val_mae: 8.2782 - val_mse: 105.2525\n",
      "Epoch 95/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.5665 - mae: 8.2651 - mse: 103.5665 - val_loss: 105.7200 - val_mae: 8.2371 - val_mse: 105.7200\n",
      "Epoch 96/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1405 - mae: 8.2287 - mse: 103.1405 - val_loss: 107.0957 - val_mae: 8.1334 - val_mse: 107.0957\n",
      "Epoch 97/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2780 - mae: 8.2422 - mse: 103.2780 - val_loss: 105.2992 - val_mae: 8.2848 - val_mse: 105.2992\n",
      "Epoch 98/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2970 - mae: 8.2375 - mse: 103.2970 - val_loss: 106.7699 - val_mae: 8.1614 - val_mse: 106.7699\n",
      "Epoch 99/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.2544 - mae: 8.2409 - mse: 103.2544 - val_loss: 105.7850 - val_mae: 8.1716 - val_mse: 105.7850\n",
      "Epoch 100/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3098 - mae: 8.2264 - mse: 103.3098 - val_loss: 105.3471 - val_mae: 8.2332 - val_mse: 105.3471\n",
      "Epoch 101/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0578 - mae: 8.2401 - mse: 103.0578 - val_loss: 105.8920 - val_mae: 8.1722 - val_mse: 105.8920\n",
      "Epoch 102/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1731 - mae: 8.2407 - mse: 103.1731 - val_loss: 105.2747 - val_mae: 8.2999 - val_mse: 105.2747\n",
      "Epoch 103/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9218 - mae: 8.2314 - mse: 102.9218 - val_loss: 105.9566 - val_mae: 8.1812 - val_mse: 105.9566\n",
      "Epoch 104/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2832 - mae: 8.2372 - mse: 103.2832 - val_loss: 105.2231 - val_mae: 8.2543 - val_mse: 105.2231\n",
      "Epoch 105/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0685 - mae: 8.2336 - mse: 103.0685 - val_loss: 105.1516 - val_mae: 8.2466 - val_mse: 105.1516\n",
      "Epoch 106/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0154 - mae: 8.2354 - mse: 103.0154 - val_loss: 105.5558 - val_mae: 8.2283 - val_mse: 105.5558\n",
      "Epoch 107/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8789 - mae: 8.2207 - mse: 102.8789 - val_loss: 105.4798 - val_mae: 8.2162 - val_mse: 105.4798\n",
      "Epoch 108/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2980 - mae: 8.2267 - mse: 103.2980 - val_loss: 105.4118 - val_mae: 8.2360 - val_mse: 105.4118\n",
      "Epoch 109/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2977 - mae: 8.2477 - mse: 103.2977 - val_loss: 106.2364 - val_mae: 8.1428 - val_mse: 106.2364\n",
      "Epoch 110/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1476 - mae: 8.2276 - mse: 103.1476 - val_loss: 105.6011 - val_mae: 8.1659 - val_mse: 105.6011\n",
      "Epoch 111/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9897 - mae: 8.2342 - mse: 102.9897 - val_loss: 106.0401 - val_mae: 8.2072 - val_mse: 106.0401\n",
      "Epoch 112/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0404 - mae: 8.2306 - mse: 103.0404 - val_loss: 105.5387 - val_mae: 8.1904 - val_mse: 105.5387\n",
      "Epoch 113/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1315 - mae: 8.2429 - mse: 103.1315 - val_loss: 105.6643 - val_mae: 8.2325 - val_mse: 105.6643\n",
      "Epoch 114/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4667 - mae: 8.2408 - mse: 103.4667 - val_loss: 105.9168 - val_mae: 8.1589 - val_mse: 105.9168\n",
      "Epoch 115/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4168 - mae: 8.2356 - mse: 103.4168 - val_loss: 105.2850 - val_mae: 8.3027 - val_mse: 105.2850\n",
      "Epoch 116/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0581 - mae: 8.2265 - mse: 103.0581 - val_loss: 105.6068 - val_mae: 8.2064 - val_mse: 105.6068\n",
      "Epoch 117/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.0449 - mae: 8.2290 - mse: 103.0449 - val_loss: 105.3889 - val_mae: 8.2610 - val_mse: 105.3889\n",
      "Epoch 118/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.3173 - mae: 8.2422 - mse: 103.3173 - val_loss: 105.5249 - val_mae: 8.2004 - val_mse: 105.5249\n",
      "Epoch 119/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0887 - mae: 8.2260 - mse: 103.0887 - val_loss: 105.5876 - val_mae: 8.2366 - val_mse: 105.5876\n",
      "Epoch 120/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0700 - mae: 8.2359 - mse: 103.0700 - val_loss: 105.2029 - val_mae: 8.2718 - val_mse: 105.2029\n",
      "Epoch 121/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.9863 - mae: 8.2271 - mse: 102.9863 - val_loss: 105.2051 - val_mae: 8.2708 - val_mse: 105.2051\n",
      "Epoch 122/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9864 - mae: 8.2374 - mse: 102.9864 - val_loss: 106.3547 - val_mae: 8.1358 - val_mse: 106.3547\n",
      "Epoch 123/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.0559 - mae: 8.2180 - mse: 103.0559 - val_loss: 105.7129 - val_mae: 8.2113 - val_mse: 105.7129\n",
      "Epoch 124/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.3050 - mae: 8.2434 - mse: 103.3050 - val_loss: 106.4087 - val_mae: 8.1500 - val_mse: 106.4087\n",
      "Epoch 125/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0564 - mae: 8.2188 - mse: 103.0564 - val_loss: 105.5027 - val_mae: 8.3824 - val_mse: 105.5027\n",
      "Epoch 126/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2179 - mae: 8.2439 - mse: 103.2179 - val_loss: 105.1401 - val_mae: 8.2673 - val_mse: 105.1401\n",
      "Epoch 127/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9201 - mae: 8.2257 - mse: 102.9201 - val_loss: 105.5890 - val_mae: 8.2038 - val_mse: 105.5890\n",
      "Epoch 128/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.2610 - mae: 8.2298 - mse: 103.2610 - val_loss: 105.4963 - val_mae: 8.1901 - val_mse: 105.4963\n",
      "Epoch 129/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.0634 - mae: 8.2382 - mse: 103.0634 - val_loss: 106.6078 - val_mae: 8.1349 - val_mse: 106.6078\n",
      "Epoch 130/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1591 - mae: 8.2227 - mse: 103.1591 - val_loss: 105.4543 - val_mae: 8.2013 - val_mse: 105.4543\n",
      "Epoch 131/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8205 - mae: 8.2292 - mse: 102.8205 - val_loss: 105.4110 - val_mae: 8.2147 - val_mse: 105.4110\n",
      "Epoch 132/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9442 - mae: 8.2203 - mse: 102.9442 - val_loss: 105.4994 - val_mae: 8.2099 - val_mse: 105.4994\n",
      "Epoch 133/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1620 - mae: 8.2471 - mse: 103.1620 - val_loss: 106.6078 - val_mae: 8.1277 - val_mse: 106.6078\n",
      "Epoch 134/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.7825 - mae: 8.2197 - mse: 102.7825 - val_loss: 105.1393 - val_mae: 8.2555 - val_mse: 105.1393\n",
      "Epoch 135/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9901 - mae: 8.2189 - mse: 102.9901 - val_loss: 105.6260 - val_mae: 8.1849 - val_mse: 105.6260\n",
      "Epoch 136/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.7338 - mae: 8.2137 - mse: 102.7338 - val_loss: 106.4732 - val_mae: 8.1383 - val_mse: 106.4732\n",
      "Epoch 137/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.8101 - mae: 8.2293 - mse: 102.8101 - val_loss: 105.3736 - val_mae: 8.2311 - val_mse: 105.3736\n",
      "Epoch 138/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0938 - mae: 8.2396 - mse: 103.0938 - val_loss: 105.5227 - val_mae: 8.2193 - val_mse: 105.5227\n",
      "Epoch 139/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2848 - mae: 8.2488 - mse: 103.2848 - val_loss: 105.2980 - val_mae: 8.2831 - val_mse: 105.2980\n",
      "Epoch 140/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9049 - mae: 8.2343 - mse: 102.9049 - val_loss: 106.1278 - val_mae: 8.1570 - val_mse: 106.1278\n",
      "Epoch 141/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2970 - mae: 8.2190 - mse: 103.2970 - val_loss: 105.4889 - val_mae: 8.2803 - val_mse: 105.4889\n",
      "Epoch 142/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1420 - mae: 8.2401 - mse: 103.1420 - val_loss: 105.2690 - val_mae: 8.2633 - val_mse: 105.2690\n",
      "Epoch 143/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2983 - mae: 8.2492 - mse: 103.2983 - val_loss: 105.8008 - val_mae: 8.1776 - val_mse: 105.8008\n",
      "Epoch 144/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.0336 - mae: 8.2221 - mse: 103.0336 - val_loss: 105.2480 - val_mae: 8.2621 - val_mse: 105.2480\n",
      "Epoch 145/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.4109 - mae: 8.2531 - mse: 103.4109 - val_loss: 105.6074 - val_mae: 8.1965 - val_mse: 105.6074\n",
      "Epoch 146/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.7300 - mae: 8.2182 - mse: 102.7300 - val_loss: 105.4527 - val_mae: 8.2983 - val_mse: 105.4527\n",
      "Epoch 147/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8191 - mae: 8.2251 - mse: 102.8191 - val_loss: 105.4284 - val_mae: 8.3116 - val_mse: 105.4284\n",
      "Epoch 148/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3009 - mae: 8.2559 - mse: 103.3009 - val_loss: 105.4654 - val_mae: 8.1940 - val_mse: 105.4654\n",
      "Epoch 149/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9690 - mae: 8.2284 - mse: 102.9690 - val_loss: 105.4411 - val_mae: 8.2112 - val_mse: 105.4411\n",
      "Epoch 150/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9630 - mae: 8.2245 - mse: 102.9630 - val_loss: 106.1141 - val_mae: 8.1905 - val_mse: 106.1141\n",
      "Epoch 151/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.7798 - mae: 8.2104 - mse: 102.7798 - val_loss: 105.2364 - val_mae: 8.2418 - val_mse: 105.2364\n",
      "Epoch 152/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8098 - mae: 8.2350 - mse: 102.8098 - val_loss: 106.2350 - val_mae: 8.1803 - val_mse: 106.2350\n",
      "Epoch 153/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.3084 - mae: 8.2346 - mse: 103.3084 - val_loss: 105.4747 - val_mae: 8.2913 - val_mse: 105.4747\n",
      "Epoch 154/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1803 - mae: 8.2346 - mse: 103.1803 - val_loss: 106.3240 - val_mae: 8.1895 - val_mse: 106.3240\n",
      "Epoch 155/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9545 - mae: 8.2302 - mse: 102.9545 - val_loss: 106.3092 - val_mae: 8.1723 - val_mse: 106.3092\n",
      "Epoch 156/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1138 - mae: 8.2169 - mse: 103.1138 - val_loss: 105.4160 - val_mae: 8.2550 - val_mse: 105.4160\n",
      "Epoch 157/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9202 - mae: 8.2470 - mse: 102.9202 - val_loss: 105.6061 - val_mae: 8.2122 - val_mse: 105.6061\n",
      "Epoch 158/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.3170 - mae: 8.2308 - mse: 103.3170 - val_loss: 105.2998 - val_mae: 8.2232 - val_mse: 105.2998\n",
      "Epoch 159/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1340 - mae: 8.2426 - mse: 103.1340 - val_loss: 105.3401 - val_mae: 8.2176 - val_mse: 105.3401\n",
      "Epoch 160/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.9038 - mae: 8.2245 - mse: 102.9038 - val_loss: 105.8368 - val_mae: 8.1699 - val_mse: 105.8368\n",
      "Epoch 161/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.7801 - mae: 8.2390 - mse: 102.7801 - val_loss: 107.1682 - val_mae: 8.1212 - val_mse: 107.1682\n",
      "Epoch 162/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.0680 - mae: 8.2364 - mse: 103.0680 - val_loss: 105.5933 - val_mae: 8.1944 - val_mse: 105.5933\n",
      "Epoch 163/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.9647 - mae: 8.2114 - mse: 102.9647 - val_loss: 105.7635 - val_mae: 8.2120 - val_mse: 105.7635\n",
      "Epoch 164/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1859 - mae: 8.2450 - mse: 103.1859 - val_loss: 105.5284 - val_mae: 8.2516 - val_mse: 105.5284\n",
      "Epoch 165/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.3105 - mae: 8.2483 - mse: 103.3105 - val_loss: 106.3156 - val_mae: 8.1524 - val_mse: 106.3156\n",
      "Epoch 166/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8660 - mae: 8.2260 - mse: 102.8660 - val_loss: 105.3465 - val_mae: 8.2730 - val_mse: 105.3465\n",
      "Epoch 167/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.0696 - mae: 8.2424 - mse: 103.0696 - val_loss: 106.6568 - val_mae: 8.1653 - val_mse: 106.6568\n",
      "Epoch 168/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1246 - mae: 8.2212 - mse: 103.1246 - val_loss: 105.7995 - val_mae: 8.2456 - val_mse: 105.7995\n",
      "Epoch 169/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.7024 - mae: 8.2350 - mse: 102.7024 - val_loss: 106.1228 - val_mae: 8.1511 - val_mse: 106.1228\n",
      "Epoch 170/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.6486 - mae: 8.2150 - mse: 102.6486 - val_loss: 105.8073 - val_mae: 8.1849 - val_mse: 105.8073\n",
      "Epoch 171/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.1332 - mae: 8.2147 - mse: 103.1332 - val_loss: 105.4422 - val_mae: 8.2520 - val_mse: 105.4422\n",
      "Epoch 172/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8029 - mae: 8.2214 - mse: 102.8029 - val_loss: 105.5648 - val_mae: 8.2691 - val_mse: 105.5648\n",
      "Epoch 173/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.7958 - mae: 8.2133 - mse: 102.7958 - val_loss: 106.4186 - val_mae: 8.1470 - val_mse: 106.4186\n",
      "Epoch 174/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8021 - mae: 8.2284 - mse: 102.8021 - val_loss: 105.9568 - val_mae: 8.2066 - val_mse: 105.9568\n",
      "Epoch 175/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.8613 - mae: 8.2005 - mse: 102.8613 - val_loss: 105.2530 - val_mae: 8.2829 - val_mse: 105.2530\n",
      "Epoch 176/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.2160 - mae: 8.2606 - mse: 103.2160 - val_loss: 105.7438 - val_mae: 8.1768 - val_mse: 105.7438\n",
      "Epoch 177/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.0065 - mae: 8.2186 - mse: 103.0065 - val_loss: 105.3845 - val_mae: 8.2501 - val_mse: 105.3845\n",
      "Epoch 178/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8399 - mae: 8.2381 - mse: 102.8399 - val_loss: 105.6316 - val_mae: 8.1856 - val_mse: 105.6316\n",
      "Epoch 179/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.7107 - mae: 8.2171 - mse: 102.7107 - val_loss: 105.5500 - val_mae: 8.1830 - val_mse: 105.5500\n",
      "Epoch 180/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.0523 - mae: 8.2247 - mse: 103.0523 - val_loss: 105.5344 - val_mae: 8.2579 - val_mse: 105.5344\n",
      "Epoch 181/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.8648 - mae: 8.2345 - mse: 102.8648 - val_loss: 106.0162 - val_mae: 8.1537 - val_mse: 106.0162\n",
      "Epoch 182/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.9710 - mae: 8.2297 - mse: 102.9710 - val_loss: 105.9736 - val_mae: 8.1581 - val_mse: 105.9736\n",
      "Epoch 183/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8265 - mae: 8.2255 - mse: 102.8265 - val_loss: 105.3493 - val_mae: 8.2167 - val_mse: 105.3493\n",
      "Epoch 184/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.7879 - mae: 8.2345 - mse: 102.7879 - val_loss: 105.2486 - val_mae: 8.2026 - val_mse: 105.2486\n",
      "Epoch 185/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1184 - mae: 8.2357 - mse: 103.1184 - val_loss: 105.5086 - val_mae: 8.1944 - val_mse: 105.5086\n",
      "Epoch 186/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.8598 - mae: 8.2280 - mse: 102.8598 - val_loss: 105.4617 - val_mae: 8.1825 - val_mse: 105.4617\n",
      "Epoch 187/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.8727 - mae: 8.2248 - mse: 102.8727 - val_loss: 105.3939 - val_mae: 8.1971 - val_mse: 105.3939\n",
      "Epoch 188/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.7637 - mae: 8.2118 - mse: 102.7637 - val_loss: 105.6496 - val_mae: 8.1893 - val_mse: 105.6496\n",
      "Epoch 189/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0035 - mae: 8.2345 - mse: 103.0035 - val_loss: 105.6585 - val_mae: 8.1835 - val_mse: 105.6585\n",
      "Epoch 190/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.9223 - mae: 8.2303 - mse: 102.9223 - val_loss: 105.2218 - val_mae: 8.2460 - val_mse: 105.2218\n",
      "Epoch 191/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1212 - mae: 8.2304 - mse: 103.1212 - val_loss: 105.8030 - val_mae: 8.1789 - val_mse: 105.8030\n",
      "Epoch 192/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.7594 - mae: 8.2210 - mse: 102.7594 - val_loss: 106.0982 - val_mae: 8.1818 - val_mse: 106.0982\n",
      "Epoch 193/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 103.0269 - mae: 8.2493 - mse: 103.0269 - val_loss: 106.3521 - val_mae: 8.1477 - val_mse: 106.3521\n",
      "Epoch 194/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.9649 - mae: 8.2258 - mse: 102.9649 - val_loss: 105.5102 - val_mae: 8.2654 - val_mse: 105.5102\n",
      "Epoch 195/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8694 - mae: 8.2307 - mse: 102.8694 - val_loss: 105.7801 - val_mae: 8.1717 - val_mse: 105.7801\n",
      "Epoch 196/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 102.6018 - mae: 8.2177 - mse: 102.6018 - val_loss: 105.1037 - val_mae: 8.2445 - val_mse: 105.1037\n",
      "Epoch 197/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.8936 - mae: 8.2248 - mse: 102.8936 - val_loss: 105.4248 - val_mae: 8.1956 - val_mse: 105.4248\n",
      "Epoch 198/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.0240 - mae: 8.2285 - mse: 103.0240 - val_loss: 106.2851 - val_mae: 8.1502 - val_mse: 106.2851\n",
      "Epoch 199/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 102.7893 - mae: 8.2298 - mse: 102.7893 - val_loss: 105.7984 - val_mae: 8.2326 - val_mse: 105.7984\n",
      "Epoch 200/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 103.1158 - mae: 8.2355 - mse: 103.1158 - val_loss: 106.6036 - val_mae: 8.1455 - val_mse: 106.6036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e3622bda30>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression', 'Depression.1']]\n",
    "y = data['Depression.2']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 127ms/step\n",
      "Predicted Depression: 11.08\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.50, 42, 30, 18]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Depression: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Depression_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "188/188 [==============================] - 4s 11ms/step - loss: 92.6210 - mae: 8.9402 - mse: 92.6210 - val_loss: 13.2656 - val_mae: 2.9989 - val_mse: 13.2656\n",
      "Epoch 2/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 12.4248 - mae: 2.7137 - mse: 12.4248 - val_loss: 3.6702 - val_mae: 1.4656 - val_mse: 3.6702\n",
      "Epoch 3/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 8.1120 - mae: 2.1792 - mse: 8.1120 - val_loss: 4.6671 - val_mae: 1.6762 - val_mse: 4.6671\n",
      "Epoch 4/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 7.5821 - mae: 2.1129 - mse: 7.5821 - val_loss: 0.6489 - val_mae: 0.6362 - val_mse: 0.6489\n",
      "Epoch 5/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 5.9547 - mae: 1.8838 - mse: 5.9547 - val_loss: 0.4132 - val_mae: 0.4673 - val_mse: 0.4132\n",
      "Epoch 6/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 7.4916 - mae: 2.1129 - mse: 7.4916 - val_loss: 0.4763 - val_mae: 0.5477 - val_mse: 0.4763\n",
      "Epoch 7/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 6.3180 - mae: 1.9393 - mse: 6.3180 - val_loss: 2.8604 - val_mae: 1.3601 - val_mse: 2.8604\n",
      "Epoch 8/200\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 7.0186 - mae: 2.0376 - mse: 7.0186 - val_loss: 1.0990 - val_mae: 0.8788 - val_mse: 1.0990\n",
      "Epoch 9/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 6.1321 - mae: 1.9190 - mse: 6.1321 - val_loss: 1.3354 - val_mae: 0.8727 - val_mse: 1.3354\n",
      "Epoch 10/200\n",
      "188/188 [==============================] - 2s 9ms/step - loss: 5.8954 - mae: 1.8665 - mse: 5.8954 - val_loss: 0.6922 - val_mae: 0.7105 - val_mse: 0.6922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e35cc28d90>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#good one\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Dense, Dropout, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load data \n",
    "data = pd.read_csv('NewData.csv')\n",
    "\n",
    "# # Normalize features\n",
    "# data = (data - data.mean()) / data.std()\n",
    "\n",
    "# Split into train and test\n",
    "X = data[['RMT %', 'AGE', 'Depression', 'Depression.1', 'Depression.2']]\n",
    "y = data['Depression.2']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y) \n",
    "\n",
    "# Deep neural network model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(256, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(512))  \n",
    "model.add(BatchNormalization()) \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(256))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "            \n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mse'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "model.fit(X_train, y_train, validation_split=0.2, \n",
    "          epochs=200, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n",
      "Predicted Depression: 10.56\n"
     ]
    }
   ],
   "source": [
    "x_in = [[0.50, 42, 30, 18, 11]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Depression: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ritesh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save('Depression_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 124ms/step\n",
      "Predicted Depression: 10.56\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('Depression_3.h5')\n",
    "\n",
    "x_in = [[0.50, 42, 30, 18, 11]] \n",
    "pred = model.predict(x_in)\n",
    "print(f'Predicted Depression: {pred[0][0]:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from keras.models import load_model\n",
    "loaded_model1 = load_model('Depression_1.h5')\n",
    "loaded_model2 = load_model('Depression_2.h5')\n",
    "loaded_model3 = load_model('Depression_3.h5')\n",
    "\n",
    "def depression1(age, rmt, dep):\n",
    "    x_in = np.array([[rmt, age, dep]])\n",
    "    pred = loaded_model1.predict(x_in)\n",
    "    processed_pred = int(pred[0][0])\n",
    "    return processed_pred\n",
    "\n",
    "def depression2(age, rmt, dep, dep1):\n",
    "    x_in = np.array([[rmt, age, dep, dep1]])\n",
    "    pred = loaded_model2.predict(x_in)\n",
    "    processed_pred = int(pred[0][0])\n",
    "    return processed_pred\n",
    "\n",
    "def depression3(age, rmt, dep, dep1, dep2):\n",
    "    x_in = np.array([[rmt, age, dep, dep1, dep2]])\n",
    "    pred = loaded_model3.predict(x_in)\n",
    "    processed_pred = int(pred[0][0])\n",
    "    return processed_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 126ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depression1(42, 0.5, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
